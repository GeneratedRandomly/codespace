# 人工智能导论-拼音输入法
### 仲嘉暄 计35 2023010812
## 如何交互？
输入输出重定向以后，默认是二元模型，二元平滑系数为 1、没有正确率统计信息的。  
使用 `-e` 可以扩张到三元模型，此时三元、二元平滑系数都默认为 1.
使用 `-a` 可以在输出之后附加正确率统计信息。
使用 `-2` 后面跟小数指定二元平滑系数（0 到 1 之间）、`-3` 指定三元平滑系数。如果指定的系数超过范围会自动归正。
## 实验环境
本实验在Linux环境（Ubuntu 22.04）内的Python 3.13.2解释器环境下进行。使用的库可参考[requirements.txt](requirements.txt)
## 语料库和数据预处理
本实验使用了新浪2016年语料库。  
首先，[pinhan_to_npz.py](src/pinhan_to_npz.py)会读取[拼音汉字表](data/拼音汉字表.txt)，将其清洗为两个字典，放在[pin_han_table](pin_han_table)文件夹里：一个是以拼音为键、其同音字列表为值的拼汉表，存储为[pinyin_hanzi_table.npz](pin_han_table/pinyin_hanzi_table.npz)；另一个是以汉字为键、其所有拼音列表为值的汉拼表，存储为[hanzi_pinyin_table.npz](pin_han_table/hanzi_pinyin_table.npz)。  
然后，[corpus_washer.py](src/corpus_washer.py)会将[新浪语料库](corpus/sina_news_gbk)清洗干净，将其中每个文件以非中文字符分割，除去单字短句，然后存到一个 `set` 里面进行去重，最后把这个 `set` 转换为 `list` 作为npz文件存储在[washed](washed)文件夹。  
[count.py](src/count.py)会把[washed](washed)文件夹里每个文件读取，计算每个汉字一元/多元组的出现次数，存放在[frequency](frequency)文件夹里，即[one.npz](frequency/one.npz)、[two.npz](frequency/two.npz)、[three.npz](frequency/three.npz)等。
## 二元模型
### 基本思路
我们希望找到输入一串拼音之后，其所有可能代表的句子中出现概率最大的一个。我们可以认为某个字只和其前面一些字有关，整个$n$字长度句子的概率也就是在前$n-1$个字确定以后出现第$n$个字的概率，即我们设每个字分别为$w_1,w_2,w_3...w_n$，则整个句子的概率为
$$
P(w_1w_2w_3...w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_n|w_1w_2w_3...w_{n-1})
$$
我们可以进一步简化问题，认为每个字出现的概率仅和前一个字有关，即
$$P(w_1w_2w_3...w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})
$$
这就成为了二元模型。
### 公式推导
我们需要最大化
$$P(w_1w_2w_3...w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})$$
即最大化
$$\log P(w_1w_2w_3...w_n)=\log P(w_1)+\log P(w_2|w_1)+\log P(w_3|w_2)...+\log P(w_n|w_{n-1})$$  
这里，我们让$P(w_i)=\text{count}(w_i)$，用每个字在语料库中出现的总次数作为它们的概率。对于二元概率
$$
P(w_i|w_{i-1})=\dfrac{P(w_{i-1}w_i)}{P(w_{i-1})},P(w_{i-1}w_i)=\text{count}(w_{i-1}w_i)
$$
用汉字二元组在语料库中出现的总次数除以第一个字的总次数作为概率。  
如果某个概率没找到，那就很有可能是生僻字或生僻二元组，可以通过返回一个极其小的惩罚概率 `PUNISH` 来规避。  
当然，我们可以设置平滑参数$x\approx1$
$$
P(w_i|w_{i-1})\approx xP(w_i|w_{i-1})+(1-x)P(w_i)
$$  
如果要扩充到三元的情况，推广是显然的。
### 算法原理
运用Viterbi算法来求解最优的汉字序列，通过保存每一步的最优路径，逐步递推得到全局最优路径。  
具体步骤如下：
- 初始化：针对第一个拼音，找出其对应的所有可能汉字，计算每个汉字的概率，并将其作为初始状态。
- 递推：从第二个拼音开始，对于每个拼音对应的每个可能汉字，遍历前一个拼音对应的所有可能汉字，计算从每个前一个汉字转移到当前汉字的概率，选取概率最大的路径作为最优路径，并保存该路径。
- 终止：在处理完所有拼音后，找出最后一个拼音对应的所有可能汉字中概率最大的汉字，回溯其保存的路径，即为最优的汉字序列。
## 实验效果
### 准确率
纯粹的二元模型的准确率情况为（无平滑，纯二元模型）：
~~~
Line Accuracy: 40.12%
Character Accuracy: 85.46%
~~~
是在[viterbi.py](src/viterbi.py)内实现的算法在 `PUNISH = 1e-7`，平滑参数为1的情况下得到的。
### 数据预处理耗时
使用 `tqdm` 进度条无显著影响：
~~~
PinHan Time: 0.04s
Washer Time: 38.01s
Count Time: 103.99s
~~~
分别为[pinhan_to_npz.py](src/pinhan_to_npz.py)、[corpus_washer.py](src/corpus_washer.py)、[count.py](src/count.py)耗时。
在产生三元词频表时，`Count Time` 会跃升至 `309.80s`。
### `viterbi.py` 对 `input.txt` 运行耗时
使用 `tqdm` 进度条无显著影响：
~~~
Viterbi Time: 3.34s
~~~
由于这里会把 `Counter` 加载到内存中，需要约 13 秒的时间。
## 效果分析
### 好例子
术语多的句子比较容易正确：
~~~
青山绿水就是金山银山
走中国特色社会主义道路
互联网产业有巨大潜力
中国贫困地区实现网络服务全覆盖
经济建设和文化建设突出了十八大精神的重要性
~~~
此外，也有一些句子运气比较好，与词频比较有默契：
~~~
我和你吻别
人被杀就会死
因为天气刚刚好
毕竟老夫也不是什么恶魔
~~~
### 坏例子（上面是输出，下面是答案）
没什么逻辑的句子
~~~
文言文样例我死了
文言文杨利沃斯勒
~~~
被其它常用组合覆盖的情况
~~~
北京是首个举办过夏奥会与冬奥会的城市
北京市首个举办过夏奥会与冬奥会的城市

每隔四年一次的冬奥会在今年召开了
每个四年一次的冬奥会在今年召开了
~~~
我没有给二元词组配套拼音导致的问题（可以通过 `pypinyin` 来优化）
~~~
清仓大甩卖
青藏大甩卖
~~~
语法用虚词也是容易出错的地方
~~~
苹果又出新产品了
苹果有出新产品了

习近平作出重要批示
习近平做出重要批示
~~~
## 思考题
### 语料编码 
   1. GBK是汉字编码国家标准之一，主要用于处理中文，UTF-8包含了几乎所有已知语言的字符。
   2. GBK采用固定的双字节编码，每个汉字都用两个字节来表示。UTF-8用一个字节编码 ASCII 字符（如英文字母、数字和常见符号）， 一般用 3 个字节编码中文。
   3. GBK主要在中国国内使用，在处理中文时效率更高，但在国际上的兼容性较差。UTF-8国际通用，是互联网上最常用的编码格式。
   4. 其他编码方式及应用场景
      1. ASCII：美国信息交换标准代码，是最早的字符编码标准，使用 7 位二进制数来表示 128 个字符，主要用于表示英文字母、数字和一些常见符号。
      2. UTF-16 和 UTF-32：前者用两个（扩展平面四个）、后者用四个字节表示所有的符号。这两种符号用的比较少。
      3. ISO-8859-1：也称为 Latin-1，把 ASCII 后面 128 个字符的位置用来存储拉丁字母，以表示西欧语言的字符。它是 ISO 标准下的字符编码集，在欧洲的一些网站和软件中仍然有使用。
      4. Big5：是针对繁体中文设计的字符编码，主要在使用繁体中文的港台地区广泛应用。
      5. 此外，还有用于编码日语的 Shift-JIS、编码韩语的 EUC-KR 等。
### 算法设计
输入 $n$ 个拼音，每个拼音对应的汉字个数由均匀分布可知为 $V/c$，其中 $c$ 为读音数量。  
外层循环遍历输入句子的每个字，共 $n$ 次；内层循环需要考虑前一个字的所有可能情况和当前字的所有可能情况。对于每个位置，前一个字有 $V/c$ 种可能，当前字也有 $V/c$ 种可能。因此，内层循环的时间复杂度是 $O(V)\times O(V)=O(V^2)$。外层循环执行 $n$ 次，所以总的时间复杂度是 $O(nV^2)$。  
需要保存每个状态的概率和路径信息。在每一步，需要为每个可能的字保存概率和路径，每个位置有 $V/c$ 种可能的字，总共需要保存 $n$ 个位置的信息。再加上二元词频占的空间，总共为 $O(nV+V^2)$。
### 输入输出
两种代码的区别：  
代码 A 采用边读入边输出的方式，每次从标准输入读取一个数据后，立即将其输出到标准输出。代码 B 先将所有输入的数据存储在一个列表中，等到所有数据都读取完毕后，再依次输出列表中的所有元素。  
我认为，代码 A 正确。它能实时响应输入，这一点能够消除 B 输出的耦合性，更加正确。虽然 B 也不能说完全错误，但肯定比 A 差，如果遇到硬件错误或者其他问题就会输出差错。所以，若有且仅有一个正确，我选 A。否则，可以认为都对但 B 更差。
## 三元模型实现
### 原理
我们认为每一个字由前两个字决定，即有：  
$$
P(w_i|w_{i-2}w_{i-1})= \dfrac{\mathrm{count}(w_{i - 2} w_{i - 1} w_i)}{\mathrm{count}(w_{i - 2} w_{i - 1})}
$$
做平滑就有
$$P(w_i|w_{i-2}w_{i-1})=y\dfrac{\mathrm{count}(w_{i-2}w_{i-1}w_i)}{\mathrm{count}(w_{i-2}w_{i-1})}+(1-y)[xP(w_i|w_{i-1})+(1-x)P(w_i)]
$$
其中$x,y\approx1$。
### 效果
在产生三元词频表时，数据清洗需要额外约 200 秒时间。  
在运行 `viterbi` 算法时，为了把 `Counter` 加载到内存中，需要额外约 13 秒的时间。  
这样，正确率达到了  
$x=1,y=0.9$
~~~
Line Accuracy: 54.89%
Character Accuracy: 85.53%
~~~
$x=y=1$
~~~
Line Accuracy: 55.29%
Character Accuracy: 85.41%
~~~
跟二元相比
~~~
Line Accuracy: 40.12%
Character Accuracy: 85.46%
~~~
行准确度上升了，这是由于三元模型更好；字准确度变化不大，因为生僻字问题抵消了三元改进。下面在分析部分讨论。  
举一些例子看看：  
上一行三元模型比下一行二元模型好的例子：
~~~
以习近平同志为总书记的党中央
以习近平同志为总书记得当中央

清仓大甩卖
青藏大甩卖

以办好让人民满意的教育为宗旨
一般好让人民满意的教育为宗旨

学会处理人际关系
学会处理人机关系

深受广大女性的欢迎和好评
深受广大女性的欢迎合好评

人机交互与识别
人机叫呼吁市别

西方国家应对疫情措施不够
西方国家应对以清措施不够

想着过去所经历的一切
想着过去所经理的一切
~~~
三元模型的生僻字问题：（上一行三元，下一行二元）
~~~
主路修桥戤羔蝼糌扪舔硖诹
主路修桥改高楼咱们天下走

北京大学毕黥骅答靴耗
北京大雪碧清华大学好

疫情面前重腮冫椁仔糍瀣熟工科实践
疫情面前中塞两国再次携手工课时间
~~~
### 分析
三元模型在一部分语法词、术语上比二元模型好，因为它可以使用更多的信息，而且连词在二元模型容易被前一个字覆盖（欢迎和好评，被迎合这个同音词覆盖），三元模型可以规避这一问题。  
但是，三元模型生成的句子里生僻字明显增多了。这是由于语言的局部性，在约$7000^3$的三元组中，有很多组出现的频率很低，而 `viterbi` 具有一定贪心性，可能在某个字选择了概率最大的三元组，而之后按照拼音生成的可能三元组都没什么意义，反而不如当初选一个差一点点的字，之后一直可以生成很好的三元组。
### 改进方案
现在我们在每层的每个可能汉字只能保留一条路径，所以很可能会落入陷阱。如果我们保留 k 条可能性前 k 的路径，就可以改善这一问题。当然，现实里遇到上述问题的概率比较小，因为人在使用输入法的时候会自然分词，存在大量隐含信息（如何分词断句），自然分词具有比较大的实际意义，一般可以很好地撞上词频统计比较多的三元组。
### 其他模型的前景
在本实验的输入输出格式下，分词的发挥会受到影响，因为需要更多计算来考虑对拼音或者其对应的可能汉字队列如何分词，所以我没有进行实现。  
如果想加快 `viterbi` 可以使用多线程或者每层只保留概率大小前五分之一的字，但 `viterbi` 本来也不是时间瓶颈，意义不大。  
## 调参
$y$ 为三元组贡献占比，列 $x$ 为二元组贡献占比，如之前所说。当 $y=0$ 时，三元模型退化到二元（由于具体实现，可能与二元的准确率有所不同）  
| x/y | 0         | 0.1       | 0.2       | 0.3       | 0.4       | 0.5       | 0.6       | 0.7       | 0.8       | 0.9       | 1         |
| --- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |
| 0.7 |           |           |           |           |           |           |           |           |           |           | 3114/7754 |
| 0.8 |           |           |           |           |           |           |           |           |           | 1158/6914 | 3134/7761 |
| 0.9 |           |           |           |           |           |           |           | 1158/6915 | 1158/6917 | 1158/6917 | 3114/7754 |
| 1   | 4391/8274 | 4850/8385 | 5110/8452 | 5250/8490 | 5429/8514 | 5449/8507 | 5449/8518 | 5489/8561 | 5489/8565 | 5489/8553 | 5529/8541 |

以上表格纵坐标为 $x$，横坐标为 $y$，内容为$行正确率/字正确率$，用四位小数表示。  
总体上看有三个特征：
- 尽可能使用三元组、二元组的正确率最高
- 当三元组贡献下降，正确率显著下降且降速递增，但仍能保持不错的水平
- 二元组贡献下降，正确率会急剧下降

对应的原因为：
- 使用最多信息，正确率最高
- 由于三元组的稀疏性，其对正确率的影响较小
- 即使三元组正常工作，二元组也要参与第二个拼音的确定，并且会对三元组起到平滑作用，二元组的系数降低会影响这些功能。
## 其他评价指标
生僻字个数：在输出文件和答案文件中查看每个字的总字频，如果低于阈值（如 50）就视为生僻字，比较两个文件的生僻字个数。在阈值为50的情况下（可在[test.py](src/test.py)第 7 行修改），`output_rare_chars=35, answer_rare_chars=0`。
## 其他
### Honor Code
参考了[学长代码](https://github.com/zhaochenyang20/IAI_2022/tree/main/homework/input-method#%E6%AD%A3%E7%A1%AE%E6%A1%88%E4%BE%8B)，学到了 `.npz` 文件保存、`Counter`、`tqdm`、`Pathlib`、`argparse` 等等，使我事半功倍。  
同时，使用了 Github Copilot。
### 其他
断断续续写了一周多，现在回头看才感觉其实难度还可以。不过如果实验文档能够稍微介绍一些工程上的技巧和思路，应该会让同学们做的更快。其实算法本身的难度没那么大，尤其是在 `Python` 这种灵活的语言里，还有 Copilot 的辅助，主要时间都花在工程上面了。自己做出成果的时候还是很有成就感的。现在回头看，失败的尝试主要在 OJ 提交上，我是因为一开始没有很好地复刻课件内容（比如，没用 log 而是直接乘积）导致的。还有一部分失败在一些工程欠考虑之处。比如有好几次输出的全都是生僻字，加了惩罚也没用，打开文件资源管理器发现是词频表出了问题，都只有 1 kb，回想是在删去词频表中的生僻字的时候把逻辑写错了，导致要重新清洗数据。随着问题的解决，犯的错误也就越来越少了。